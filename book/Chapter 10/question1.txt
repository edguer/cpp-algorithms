It all depends on the volume of data and how it is used, but let's take Brazilian's example: B3 holds stocks for 400 companies, excluding other types of assets. Let's supposed to total number of assets is 4,000: it is still not a lot of assets, something we could hold in one machine, depending on the size. Of course, we can add more machines if we want reliability or scalability - the problem statement does not mention them. Let's suppose a first scenario, then, in which we want to optimize our resources, meaning we should have minimal number of machines. What is the hardware configuration? [supposing 12GB mem, good CPU] In this case, caching the 4,000 assets into one machine's memory will allow us to serve more than a 1,000 requests per second. If we can serve more than 1,000 transactions per second will depend on network latency. One way of caching the data is through pre-processing: at the end of the day, we read the data from it's source (let's say, a Cosmos DB) and dump it all to the machine's cache. If any request comes in and the data is not available in memory yet, we can go directly to Cosmos DB. 

Well, this is for a simple scenario. Let's suppose we have more data, much more: 2 or 3 million assets. In this case, we will need more machines, more processing power and memory. If we keep with 1,000 users, we won't want to hold everything in cache, since we may expect most of the assets won't be accessed (except if each user/application needs information about 3k assets everyday), so we could build a lazy cache system, that only caches data as they are requested. Additionally, some data can be pre-fetched and cached beforehand, based on utilization data: let's say PETR4 is always requested, everyday, so we cache it before anybody needs it, since we know a lot of people will ask for it. This will definetly consume more CPU and proportionally less memory, also some requests will take longer - if data is not in the cache. Also, we will have duplicated cache entries on both machines and for each asset, we will need a couple of "slower" execution, one for each machine, since we need to build the cache in both. To solve this, there are a couple of alternatives: using a pub/sub via RPC between the machines, if posssible, meaning everytime a machine adds something to the cache, it notifies the other machines in the group about it, so they can also cache the same data, avoiding that another user as for the same data, in another machine, and have to wait for it to me fetched and cached. Another approach is to work with a solution like Redis cache, which would require additional machines, but then the "front-end" machines will need less memory. Now, if you absolutely need all the data to be cached, a sharding mechanism is possible, so you distributed the data accross the machines and the applications will have to go to different machines based on range or lookup tables.