A regular int is 32 bits, 4 bytes, even if unsigned. So we must consider each int will cost the regular 4 bytes in memory, and we only 1GB in RAM, 4k less than we need. 
Also, if we assume there is no dots between numbers, we can say a single number read will take actually 10 bytes in memory - we read in chars, one byte each, to than convert to a 4 bytes integer.

One feasible way would be using external merge sort: we read the file in chunks of less than 60% of available memory, for example, so we spare the memory to drop all the integer in memory. A sorting algorithm can put the numbers in order, to prepare ourselves to an external merge sort. After all files are generated, in an ordered way, we start the merge sort: open as much files as the memory allows at once, reading only the first ints, meaning that, if I have 4k files, I will load 4k * 14 bytes, which fits in 1GB. We find the lower value of those and compare to the lowest value found - which starts with -1. If the value found is higher than lowest + 1, than the value is lowest + 1, otherwise the algorithm continues by only moving on the file that last presented the lowest value between than, and so on. 

With less memory, we need to use more disk. As we expect to have more files, reading those ordered files and comparing their content might be a challenge. For example, we could not be able to open all files at once, so we must adopt a different strategy: instead of opening all at once, we need to open just few of them, let's say 10, and read them at the same pace until, let's say, offset 5,000, and keep track of unvisited numbers until the highest value found on those files. Next, we read +10 files, many numbers will be visited - integers are ordered - and others will be added. We do that until the higher unvisited value is lower than the lowest value of any of the files in the set.