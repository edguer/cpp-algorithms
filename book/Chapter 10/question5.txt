Crawling pages is very similar to graph problems, so we would have to keep a log of already visited pages to avoid infinte loops. The problem memory limitation, depending on the scenario at hand. For example, for an intranet of a medium size company, it is highly probable that a single server could handle the crawling process all in itself, so the visited pages logs would be all in memory. For higher volume, though, the crawling process should be distributed accross multiple servers, each one with a different feed or entrypoint, but with a single visited pages list. To solve that problem, we have to put that list in an external data source, most probably using disk storage, which is slower, but then we can scale. To make it faster, a cache mechanism with a small TTL on that data source could be used, so frequently visited pages will be available at hand (some pages tend to have more links than others, such as facebook or instagram, so keeping them in the cache would be a good idea). 